---
title: "Reading cleaning data"
author: "Guillaume Barbalat"
date: "April/2022"
output: html_document
---

summary
1- data reading and cleaning, with paralleling of your analysis
2- ggplot2
3- table 1
4- flowchart
5- accessing data on the internet
6- gam and bayesian with INLA
7- SuperLearner
8- CEA
9- tmle
10- lmtp

# 1-data reading and cleaning

## a-reading files, paralleling

```{r}
rm(list=ls())
library(dplyr)
library(ggplot2)
library(doParallel)

here_data="G:/_DATA/"
outcome_has_been_loaded=0
##########
#Y- DALYs read
##########
here_data_DALYs=paste0(here_data,"all_data_files_GBD/")
list_files=list.files(path=here_data_DALYs, pattern =
                        glob2rx("IHME-GBD_2019_DATA-*\\csv"))#

read.csv("G:/_DATA/all_data_files_GBD/population/IHME_GBD_2019_POP_1990_Y2020M10D15.csv",
         sep=";") %>%
  dplyr::select(where(is.character))  %>%
  lapply(table)
         

if (outcome_has_been_loaded==0) {

  outcome_matrix=list()

  num_cores <- detectCores()
  cl <- makeCluster(num_cores-1)
  clusterEvalQ(cl,library(dplyr))#c(library(SuperLearner),library(dplyr))
  clusterExport(cl, list("here_data_DALYs","list_files"))

  registerDoParallel(num_cores-1)
  getDoParWorkers()


  try_this <- function(i) {

    tmp=read.csv(paste0(here_data_DALYs,list_files[i]))
    outcome_matrix <- tmp %>%
      select(-c("upper","lower"))

    return(outcome_matrix)
  }


  system.time(outcome_matrix <-clusterApply(cl,1:length(list_files),try_this))

  stopCluster(cl)

  outcome_matrix_=as_tibble(bind_rows(outcome_matrix))
}
```


## b-merging, selecting, filtering, relocating, wide to long

```{r}
ideal_matrix<-data.frame(location=unique(col1$c1)) %>%
  left_join(data.frame(year=unique(col2$c2)), by=character())

not_ideal_matrix <- ideal_matrix[-sample(1:nrow(ideal_matrix), 10), ]
not_ideal_matrix$y <- 4

#create an ideal matrix
joined <- ideal_matrix %>%
  left_join(not_ideal_matrix,by=c("location","year"))
colSums(is.na(joined))
joined[which(is.na(joined$y)),]


load("G:/PROJECTS/2022/sAP Globalization/OUD.RData")

outcome_matrix_ %>%
  # filter(age=="25 to 29") %>%
  dplyr::filter(location=="Austria") %>%
  filter(!grepl('ARC', Study.Subject.ID)) %>% #row contains this ...
  dplyr::select(where(is.character))  %>%
  select(starts_with)#contains, #matches
  lapply(table)

  #filter, mutate, case_when, group_by and create a column that takes into account the grouping
final_matrix_final <-  final_matrix_2019 %>% 
  filter(Quality>=3) %>%
  mutate(Quality = Quality - 3,
         bin_KOFGI=case_when(
         KOFGI>median(KOFGI, na.rm=TRUE) ~ 1,
         KOFGI<=median(KOFGI, na.rm=TRUE) ~ 0 )
         ) %>%
  
  group_by(bin_KOFGI) %>%
  mutate(KOFGI_ctrd=scale(KOFGI,scale = FALSE)) %>%
  relocate(KOFGI_ctrd, .after = KOFGI)

anti_join #one but not the other

library(tidyr)
remain_ %>%
  pivot_longer(cols=everything(),
               names_to="set",
               values_to="LISTE_DOMAINE_SOIN",
               values_drop_na=TRUE
               )

#also possible to do pivot_wider

```

## c-conversion numeric to factors, ordered factors

```{r}
#non-numeric to numeric
temperature_vector <- c("High", "Low", "High", "Low", "Medium")
temperature_vector_num <- factor(temperature_vector,
                                 #order=TRUE,
                                 levels = as.character(unique(temperature_vector)))
as.numeric(temperature_vector_num);

#char to ordinal
temperature_vector <- c("High", "Low", "High", "Low", "Medium")
factor_temperature_vector <- factor(temperature_vector, 
                                    order = TRUE, #this makes it an ordinal variable
                                    levels = c("Low", "Medium", "High"))

#numeric to ordinal
temperature_vector <- c(3, 1, 3, 1, 2)
factor_temperature_vector <- factor(temperature_vector, 
                                    order = TRUE, #this makes it an ordinal variable
                                    levels = c(1, 2, 3))

```

## d-create dummies

```{r}
library(fastDummies)
dummy_cols(
  .data,
  select_columns = NULL,
  remove_first_dummy = TRUE,#to avoid multicol
  remove_most_frequent_dummy = FALSE,
  ignore_na = FALSE,
  split = NULL,
  remove_selected_columns = FALSE#removes the columns used to generate the dummy columns
)

#alternative: tribute to Susan Gruber!!!
ethnicCat <- factor(c("a", "b", "a", "b", "c"))
age <- c(31, 28, 43, 41, 50)

W <- data.frame(ethnicCat, age)

W.converted <- model.matrix(tempY ~ ., data = data.frame(tempY = 1:nrow(W), W))[, -1]
    if (is.vector(W.converted)) {
        W.converted <- as.matrix(W.converted)
    }

# at this point W.converted has two binary columns, “ethnicCatb”, “ethnicCatc”, “age”
```

## e-remove observations where N per cell is below N=5

```{r}

#select cells you want to work on
df1 <- SL_data %>% 
  dplyr::select(-StudySubjectID) %>%
  dplyr::select(where(is.factor)) 

#concatenate from list and find names of each columns
k=do.call("c",df1);names(k)=sub(".*\\.", "", names(k))

#create key to recode
tmp2=which(k < 2);tmp2[]=NA_character_

#recode with NA
if (length(tmp2)!=0) {
df2 <- df1 %>% 
  mutate(across(everything(), ~recode(., !!!tmp2)
  ))
}

#take out NA
SL_data <- SL_data[complete.cases(df2),]

#drop levels
SL_data <- SL_data %>% 
  mutate(across(where(is.factor),~droplevels(.)))
```

## f- discretize

```{r}
mutate(across(c(Age,GAF,CGI,
                  SQoL18_SEL:SQoL18_TOT,
                  WEMWBS_TOT,
                  ISMI_TOT,
                  IS_TOT
                  ), ~ cut(.x,
                           breaks = numbers_of_bins,
                           include.lowest = TRUE),
                           )) 

```

## g-change values after having discretized

```{r}

mutate(across(c(Age,GAF,CGI,
                  SQoL18_SEL:SQoL18_TOT,
                  WEMWBS_TOT,
                  ISMI_TOT,
                  IS_TOT
                  ), ~ case_when(.x==4~1,
                                 .x!=4~0)
)
)


```


## h-quantile and ecdf

``` {r}
psi_1=psi_results_data[[i]][[idx_psi1]]
  psi_2=psi_results_data[[i]][[idx_psi2]]
  psi_3=psi_results_data[[i]][[idx_psi3]]
  psi_4=psi_results_data[[i]][[idx_psi4]]
  
  #ecdf is the inverse of quantile
Fn1 <- ecdf(psi_1$density_ratios);Fn1(10)
Fn2 <- ecdf(psi_2$density_ratios);Fn2(10)
Fn3 <- ecdf(psi_3$density_ratios);Fn3(10)
Fn4 <- ecdf(psi_4$density_ratios);Fn4(10)
plot(x,Fn1(x))

Fn1 <- ecdf(psi_1$density_ratios[psi_1$density_ratios!=0]);Fn1(10)
Fn2 <- ecdf(psi_2$density_ratios[psi_2$density_ratios!=0]);Fn2(10)
Fn3 <- ecdf(psi_3$density_ratios[psi_3$density_ratios!=0]);Fn3(10)
Fn4 <- ecdf(psi_4$density_ratios[psi_4$density_ratios!=0]);Fn4(10)

trim=Fn(10)

```



## i- missing values

```{r}
###############################################################
#another strategy for missing values
###############################################################
#select everything(ends_with("_NA"))
NA_dum <- SL_data_30_MHC %>%
  select(ends_with("_NA"))

#function to find the mode
find_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
}

#replace NA by median and mode, then cbind NA_dum (sl3 package strategy)
mat_new_NA <-mat_A_W_rd_PEC_6dx_MHC_NotDummy_30 %>%

  group_by() %>%
  mutate(across(where(is.numeric),
                ~case_when(!is.na(.x)~as.numeric(.x),
                           is.na(.x)~as.numeric(median(.,na.rm=TRUE)))
  ),
  across(where(is.factor),
         ~case_when(!is.na(.x)~as.factor(.x),
                    is.na(.x)~factor(find_mode(.))
         )
  ),
  STORI=replace(STORI,is.na(STORI),"REBUILDING")#as this is the mode (not NA)
  ) %>%
  cbind(NA_dum)

```
# 2-ggplot2

```{r}
pd <- position_dodge(width = 0.3)

ggplot(data=TAB_EST,
          aes(x = Disorder,y = Estimate, 
              ymin = Estimate-1.96*SE, ymax = Estimate+1.96*SE))+#,colour="black"))+#pval>0.05 "black"
  geom_hline(aes(yintercept =0), linetype=2,linetype="dashed")+
  geom_pointrange(aes(x=Disorder))+#,colour="red" ,size=1)+#pval>0.05 "black"
  geom_point(position=pd)+
  labs(x='Disorder',y="Estimate",title = 'B. Individualism', colour="Weight truncation")+
  geom_errorbar(aes(ymin=Estimate-1.96*SE, ymax=Estimate+1.96*SE),#colour="black",#pval>0.05 "black"
                width=0.25,cex=2,, position=pd)+
  theme(legend.position = "none",
        panel.background = element_rect(fill = "white",colour = "black"),
        strip.background = element_rect(colour = "black", fill = "white"),
        axis.title.x = element_text(face = "bold",size=16),
        axis.title.y = element_blank(),#element_text(face = "bold",size=16),
        axis.text.y=element_text(face = "bold",size=16),
        axis.text.x = element_text(colour = "black", size=12),# 
        plot.title = element_text(colour = "black",face="bold", size=20)#,hjust = 0.5)#family, 
  )+
  scale_x_discrete(limits = c("eat","anx","bip","dep"),
                   labels=c("Eating\ndisorders","Anxiety\ndisorders",
                            "Bipolar\ndisorders","Depressive\ndisorders"))+
  scale_colour_manual(values = c("black", "darkblue","lightblue","grey"))+

  coord_flip()+
  #facet_wrap(~Effect)
  scale_fill_gradient2(low = "blue", high = "red") +
  theme(legend.title = element_blank(),
            legend.position = c(.95,.60),
            panel.background = element_rect(fill = "grey"),
            legend.box  = "none",
            panel.grid = element_blank())


    name.labs <- c("Male", "Female")
    names(name.labs) <- c("1", "2")
    ggplot(full_data, aes(x=factor(V2), y=val_base, fill=factor(A_base))) +
      geom_boxplot()+
      scale_x_discrete(limits=c("1","2"),
                       labels=c("15-19 y/o","20-24 y/o"))+
      facet_grid(~V1, labeller = labeller(V1 = name.labs))+ 
      labs(title="", x="Age groups", y="1996 CUD DALYs",fill="MML")+
      scale_color_discrete(name="MML")+ 
      theme(#legend.position = "none",
            panel.background = element_rect(fill = "white",colour = "black"),
            strip.background = element_rect(colour = "black", fill = "white"),
            strip.text.x = element_text(colour = "black", size=14),# face = "bold",
            axis.title.x = element_text(size=16),axis.title.y = element_text(size=16)
      )

```

# 3-table 1

```{r}
library(table1)

my.render.cont <- function(x) {
    with(stats.default(x), 
         sprintf("%0.2f (%0.1f)", MEAN, SD))
}

table1(~ AGE+SEX+EDUCATION + FAM +  DX2 + EMPLOYMENT+
         GAF+CGI+FIRST_CONTACT+
         ILLNESS_DURATION + HOSPITAL 
       | DX#*age
       , data=data,  rowlabelhead = "Variables",
       render.continuous=my.render.cont)


```

# 4-flowchart
https://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html

```{r}

library(DiagrammeR)
grViz("digraph flowchart {
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']
      tab4 [label = '@@4']
      tab5 [label = '@@5']

      # edge definitions with the node IDs
      tab1 -> tab2;
      tab2 -> tab3;
      tab2 -> tab4 -> tab5
      }

      [1]: 'Questionnaire sent to n=1000 participants'
      [2]: 'Participants came to clinic for evaluation n=700'
      [3]: 'Participants non-eligible for the study n=100'
      [4]: 'Participants eligible for the study n=600'
      [5]: 'Study sample n=600'
      ")
```


# 5-accessing data on the internet

```{r}
#packages install
{
#read 
  #https://slcladal.github.io/webcrawling.html
install.packages("rvest")
install.packages("readtext")
install.packages("webdriver")
webdriver::install_phantomjs()
# install klippy for copy-to-clipboard button in code chunks
remotes::install_github("rlesur/klippy")
}


# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
library(tidyverse)
library(rvest)
library(readtext)
library(webdriver)
# activate klippy for copy-to-clipboard button
klippy::klippy()

# define url
url <- "https://www.theguardian.com/world/2017/jun/26/angela-merkel-and-donald-trump-head-for-clash-at-g20-summit"
# download content
webc <- rvest::read_html(url)
# inspect
webc

webc %>%
  # extract paragraphs
  rvest::html_nodes("p") %>%
  # extract text
  rvest::html_text() -> webtxt
# inspect
head(webtxt)

#headline
webc %>%
  # extract paragraphs
  rvest::html_nodes("h1") %>%
  # extract text
  rvest::html_text() -> header
# inspect
head(header)

pjs_instance <- run_phantomjs()
pjs_session <- Session$new(port = pjs_instance$port)

url <- "https://www.theguardian.com/world/angela-merkel"
# go to URL
pjs_session$go(url)
# render page
rendered_source <- pjs_session$getSource()
# download text and parse the source code into an XML object
html_document <- read_html(rendered_source)

links <- html_document %>%
  html_nodes(xpath = "//div[contains(@class, 'fc-item__container')]/a") %>%
  html_attr(name = "href")
# inspect 
links


#seems like a good webpage
#https://github.com/yusuzech/r-web-scraping-cheat-sheet/blob/master/README.md
#https://resulumit.com/teaching/scrp_workshop.html#171
{
  library(RSelenium)
  library(httr)

# start the server and browser(you can use other browsers here)
rD <- rsDriver(browser=c("firefox"))

driver <- rD[["client"]]
# navigate to an URL
driver$navigate("http://ghdx.healthdata.org/gbd-results-tool")#("https://www.google.com/")
#selcet input box
element <- driver$findElement(using = "css", value = "#react-select-3--value .Select-input")
element$highlightElement()

driver$mouseMoveToLocation(webElement = element)


webElem <- driver$findElement(using = 'xpath', value = "//[(@id = "react-select-3--value")]//")
#"//span[contains(text(),'27 Cancer of ovary')]/parent::a

  #[contains(concat( " ", @class, " " ), concat( " ", "Select-input", " " ))]
#driver
driver$executeScript("arguments[0].click();", list(webElem))


#send text to input box. don't forget to use `list()` when sending text
element$sendKeysToElement(list(key="down_arrow"))
element$clickElement()
element$sendKeysToElement(list(value="Prevalence",key="enter"))
element$click(buttonId = "")


rm(rD)
gc()

#close the driver
driver$close()

#close the server
rD[["server"]]$stop()
}
```

# 6-gam

Alternatively, one can think of implementing these models using a
hierarchical Bayesian framework, with hyperparameters representing
constraints on parameter estimates. However, Bayesian inference on such latent
Gaussian models (LGM) is not straight forward since, in general, the posterior
distribution is not analytically available. Markov Chain Monte Carlo (MCMC)
techniques are the standard solution to this issue, yet they are computationally
very intensive and difficult to implement. Integrated Nested Laplace
Approximation (INLA) is a more recent tool for Bayesian inference on latent
Gaussian models when the focus is on posterior marginal distributions. INLA
substitutes MCMC simulations with accurate, deterministic approximations to
posterior distriubtions. The quality of these approximations is in general very
high, such that even very long MCMC runs could not detect any errors. According
to LGM, there is a n-dimensional field $x$. Both the covariance matrix of the
Gaussian field $x$, and the likelihood model for $y/x$ can be controlled by some
unknown hyperparameters ${\theta}$. Importantly these generalized structured
additive regression models which can take many different forms such as -
non-linear effects of continuous covariates - time trends, - seasonal effects -
iid random intercepts and slopes - group specific random effects and spatial
random effects. Linear parameters, unknown functions and . THe INLA library
requires each data point $y$ to be dependent on the latent Gaussian field only
through one single element of $x$. The hyperparameters theta are assigned a
prior distribution. Importantly, - the LGM x admits conditional independence
properties: it is a latent Gaussian Markov random field with a sparse precision
matrix. - the dimensions of the hyperparameters are small, >6 With these
conditions, INLA provides a recipe for fast Bayesian inference using accurate
approximations of marginal posterior densities of hyperparameters and for the
posterior densities for the latent variables $x$. We assume DALYs for each
mental disorders to be conditionnally independent Gaussian variables of mean
${\eta_i}$ and unknown precision (the precision matrix is the inverse of the
variance-covariance matrix) ${\tau_i}$
Let us define a first model as
$${\eta_i}={\mu}+z_i^T{\beta}+f_s(s_i)+f_u(s_i)$$ - spatially structured
component - spatially unstructured component, which is i.i.d normally
distributed with zero mean and unknown precision which is assumed to vary
smoothly from region to region. $f_u(s_i)$ is modelled as an intrinsic Gaussian
Markove random field with unknown precision, which is a conditionnally
auto-regressve prior (CAR).
The latent Gaussian field $x$ is $\{\mu;\beta;f_s;f_u;\eta_i\}$ 
The hyperparameter vector ${\theta}$ is composed by
the precision $\tau$. Vague independent Gamma priors are assigned to each element of $\theta$.

As mentioned above, in an age-period-cohort model, one wants to constrain age, period and cohort effects to solve the identifiability issues. In other words, one wishes to stipulate that the age, period and cohort effects are non-linear. The model is therefore as follows:
$${\eta_i}={\mu}+z_i^T{\beta}+f_a(age)+f_p(period)+f_c(cohort)+f_s(s_i)+f_u(s_i)$$;

Here, $f_a , f_p, f_c$ follow an instrinsic second-order random-walk model with unknown precision $\tau_a, \tau_p, \tau_c$.

Overall, to ensure the identifiability of $\mu$, a sum-to-zero constraint (`constr=TRUE`) must be imposed on all $f$

As an alternative hypothesis to explain spatial variability one could imagine that the effect of one covariate has a different slope for different regions. A weight would then be imposed on the spatial variability by what was previously the covariate. This kind of models are known as space-varying regression models.


Yet another possibility is to use a gam model where parameters could be represented in a linear way and add spatial and temporal basis functions

```{r eval=FALSE}
library(mgcv)#mgcv or gamm4
library(gamm4)

#Variables may be included based on LASSO Selection.

      #s() + stb weights + corr
      k_a2<-gamm(val_log~
                   s(V1,V2, k=-1,bs='gp', m=2)+#k=nb_df,
                   s(temporal)+
                   Joy_r+Indiv_r+#Trust_r+
                   SDI+Unemploy_r+p90p100+
                   gallup+
                   haqi+Nb_Sources+coeff_var
                 , family=gaussian()#poisson(), quasipoisson(), nb
                 #  ,correlation = corSpher(form = ~V1+V2),#location_factor|region_factor
                 #include temporal correlation structur?
                 weights = (pop/sum(pop)),#(tmp_a$pop)/(sum(tmp_a$pop))
                 na.action = na.omit,
                 data=tmp_a)
      
      # k_a4<-gamV(val_log~s(V1,V2, k=-1,bs='gp', m=2)+#k=nb_df,
      #              Joy_r+Indiv_r+#Trust_r+
      #              SDI+Unemploy_r+p90p100+
      #              gallup+
      #              haqi+Nb_Sources+coeff_var
      #            , family=gaussian()#poisson(), quasipoisson(), nb
      #            ,aGam = list(weights = (1/tmp_a$pop),#/(sum(1/tmp_a$pop)),
      #                         na.action = na.omit),
      #            data=tmp_a)
      
      print(check.gamViz(k_output[[i]][[j]]))
      influence.gam(k_output[[i]][[5]])[35]#leverage
```


# 7-SuperLearner and VIM


## 7-a SuperLearner package

```{r}

###############################################################
#prepare SL libraries using SuperLearner package
###############################################################
listWrappers()

#add on highly adaptive lasso
SL.hal9001<- function (Y, X, newX = NULL, family = "binomial", 
                    obsWeights = rep(1, length(Y)), id = NULL, max_degree = ifelse(ncol(X) >= 20,2, 3),
                    smoothness_orders = 1, num_knots = ifelse(smoothness_orders >= 1, 25, 50),
                    reduce_basis = 1/sqrt(length(Y)), lambda = NULL, 
                    ...) 
{
  if (!is.matrix(X)) 
    X <- as.matrix(X)
  if (!is.null(newX) & !is.matrix(newX)) 
    newX <- as.matrix(newX)
  hal_fit <- fit_hal(X = X, Y = Y, family = family$family, 
                     fit_control = list(weights = obsWeights), id = id, max_degree = max_degree, 
                     smoothness_orders = smoothness_orders, num_knots = num_knots, 
                     reduce_basis = reduce_basis, lambda = lambda)
  if (!is.null(newX)) {
    pred <- stats::predict(hal_fit, new_data = newX)
  }
  else {
    pred <- stats::predict(hal_fit, new_data = X)
  }
  fit <- list(object = hal_fit)
  #class(fit) <- "SL.hal9001"
  out <- list(pred = pred, fit = fit)
  return(out)
}
SL.library_my_own=c('SL.hal9001')
##########


## All screening algorithm wrappers in SuperLearner:
## [1] "All"
## [1] "screen.SIS"            "screen.corP"           "screen.corRank"       
## [4] "screen.glmnet"         "screen.randomForest"   "screen.template"      
## [7] "screen.ttest"          "write.screen.template"
SL.library= c( #"SL.randomForest", "SL.xgboost", 
               #"SL.step.forward", #"SL.stepAIC"#"SL.step.interaction",
               "SL.step.forward",
               "SL.glmnet", "SL.earth",
               "SL.mean"
               )

# #
little_SL.library="SL.glmnet"
#                

 cvControl = list(V = 2, stratifyCV=FALSE, shuffle=TRUE) 
innerCvControl = list(list(V=4))
#or you can use the standard arguments

method = "method.AUC" # "method.AUC" "method.NNloglik" "method.NNLS"
###############################################################



###############################################################
#your Y
#test one PSR method vs. another
PEC_OI=c("SOC_ASP",
         "EDUC",
         "CBT",
         "COG_REM"
)
###############################################################


###############################################################
#what you'll do
#SL WITH LASSO ONLY FOR CV
#SL WITH A BUNCH OF LIBRARIES
###############################################################

###############################################################
#cluster your analysis
###############################################################
{
  num_cores <- detectCores()
  cl <- makeCluster(num_cores-2)
  clusterEvalQ(cl,c(library(SuperLearner),library(dplyr)))
  clusterExport(cl, list("little_SL.library","SL.library","cvControl","innerCvControl","method"))
  
  try_this <- function(x,SL_data, PEC_OI) {
    
    Y<-SL_data %>%
      dplyr::select(matches(PEC_OI[x],ignore.case=FALSE)) %>%
      mutate(across(everything(.),~ replace(., is.na(.), 0) )) #comment if No exclude NA and replace by
    
    Y=ifelse(Y==1,1,0)
    
    X=select(SL_data,-c(StudySubjectID,
                        #c("ProtocolID_02_C3R_CH Alpes-Isère":ProtocolID_20_SAMSAH_ALGED),#comment if No centre dummies
                        # "ProtocolID_05_C2RP_ CH Tour de Gassies",
                        # "ProtocolID_08_CRPS/CH ROANNE",
                        # "ProtocolID_11_CESAR_SAMSAH_CH SJdD de Lyon":"ProtocolID_20_SAMSAH_ALGED",
                        #comment if No centre dummies
                        
                        # c(SQoL18_SEL_2:SQoL18_ROM_NA),#remove all susbcales from SQOL            
                        #c(SQoL18_TOT_2:SQoL18_TOT_NA),#remove sum of subscales from SQOL
                        # c(WEMWBS_TOT_2:WEMWBS_TOT_NA),#remove all things from WEMWBS_TOT
                        # c(ISMI_TOT_2:ISMI_TOT_NA),#remove all things from ISMI_TOT  
                        # #c(IS_TOT_2:IS_TOT_NA),#remove all things from IS_TOT
                        CBT,SOC_ASP,COG_REM,EDUC
    ))
    
    sl_all = CV.SuperLearner(Y = Y, X=X,
                         family = binomial(),
                         cvControl = cvControl,
                         method=method,
                         SL.library = SL.library
                         )
    
    
    cv_sl_all = CV.SuperLearner(Y = Y, X=X,
                         family = binomial(),
                         cvControl = cvControl,
                         innerCvControl=innerCvControl,
                         method=method,
                         SL.library = SL.library
                         )
    # return(list(sl_glmnet,sl_all))#,cv_sl_all
    return(cv_sl_all)
    
  }
  
  # system.time(sl_50 <-clusterApply(cl,1:4,try_this,SL_data=SL_data_50_MHC, PEC_OI=PEC_OI))
  # system.time(sl_40 <-clusterApply(cl,1:4,try_this,SL_data=SL_data_40_MHC, PEC_OI=PEC_OI))
  system.time(sl_30 <-clusterApply(cl,1:4,try_this,SL_data=SL_data_30_MHC, PEC_OI=PEC_OI))
  # system.time(sl_20 <-clusterApply(cl,1:4,try_this,SL_data=SL_data_20_MHC, PEC_OI=PEC_OI))
  # system.time(sl_allData <-clusterApply(cl,1:4,try_this,SL_data=SL_data_MHC, PEC_OI=PEC_OI))
  
  # system.time(sl_new_NA <-clusterApply(cl,1:4,try_this,SL_data=mat_new_NA, PEC_OI=PEC_OI))
  
  
  
 
  #save(file="sl_cv_MHC.RData",sl_50,sl_40,sl_30,sl_20,sl_allData)
  
  stopCluster(cl)
}
##########the end
###############################################################


```


## 7-a VIM after you've done a SL anal
```{r}
# Prediction wrapper functions for SL VIM
imp_fun <- function(object, newdata) { # for permutation-based VI scores
  predict(object, newdata = newdata)$pred
}
par_fun <- function(object, newdata) { # for PDPs
  mean(predict(object, newdata = newdata)$pred)
}

# #load results from SL analysis
# load(file="sl_cv_MHC.RData");
# 
# PEC_OI=c("SOC_ASP",
#          "EDUC",
#          "CBT",
#          "COG_REM"
# )

#big for loop for variable importance analysis
for (i in 1:length(PEC_OI)) {
  
  print(PEC_OI[i]);
  
  sl_50_glmnet=sl_50[[i]][[1]];#after a SL anal on glmnet
  
  sl_50_allAlgo=sl_50[[i]][[2]];#after a SL anal on a number of algo
  #############################
  
  
  ##############################
  #VIM with lasso
  var_imp_LASSO <- function(sl) {
    
    glmnet_1=sl$fitLibrary$SL.glmnet_All$object
    coeff_glmnet<-coef(glmnet_1,s='lambda.min')#lambda.min lambda.1se
    var_glmnet <- data.frame(name = coeff_glmnet@Dimnames[[1]][coeff_glmnet@i + 1],
                             coefficient = coeff_glmnet@x)
    return(var_glmnet)
  }
  var_imp_glmnet_50<-var_imp_LASSO(sl_50_glmnet)
  ##############################
  
  ##############################
  #VIM with SL
  {# Setup parallel backend
    cl <- makeCluster(5) # use 5 workers
    registerDoParallel(cl) # register the parallel backend
    # Permutation-based feature importance
    
    var_imp_SL <- function(SL_data,sl) {
      
      Y<-SL_data %>%
        dplyr::select(matches(PEC_OI[i],ignore.case=FALSE))%>%
        mutate(across(everything(.),
                      ~ replace(., is.na(.), 0)
        ))
      Y=ifelse(Y==1,1,0)
      
      X=select(SL_data,-c(StudySubjectID,
                          #PSR,N_PEC,
                          CBT,SOC_ASP,COG_REM,EDUC
      ))
      Interesting_features=X %>%
        select(-c("ProtocolID_02_C3R_CH Alpes-Isère":ProtocolID_20_SAMSAH_ALGED,ends_with("_NA")
        )) %>%
        colnames()
      
      var_imp <- vi(sl, 
                    method = "permute", ########can't use "model specific" VI scores with SL
                    train = X, #no train and test, beware ...
                    target = Y, 
                    feature_names = Interesting_features,
                    metric = "auc", ####### #list_metrics() --- metric = "auc", rsquared", "rmse". "mse" etc...
                    reference_class="0",
                    type = "difference", 
                    pred_wrapper = imp_fun, 
                    nsim = 5, 
                    parallel = TRUE)
      return(var_imp)
    }
    
    var_imp_sl_50<-var_imp_SL(SL_data=SL_data_50_MHC,sl=sl_50_glmnet)
    
    stopCluster(cl)
  }
}#end of big for loop


imp_fun <- function(object, newdata) { # for permutation-based VI scores
    N_fold <- length(object$AllSL)
    all_preds=vector(length=nrow(newdata))
    
    #for each fold, call predict function
    for (i in 1:N_fold) {
      SL_model=object$AllSL[[i]]
      which_row=as.integer(rownames(object$AllSL[[i]]$SL.predict))
      all_preds[which_row]=predict.SuperLearner(object=SL_model, newdata = newdata[which_row,])$pred
      #, newdata = newdata[which_row,]
    }
    
    return(all_preds)
  }
  imp_fun(object=test,newdata=X)

```

## 7-c sl3 package

```{r}
#########
#try it on with sl3 package!
#can have a multinomial Y!
##########
#################################################################
#####multinomial
#################################################################
#first transform data.frame
SL_data<-SL_data_30_MHC %>% 
  tidyr::pivot_longer(cols = c("CBT","EDUC","COG_REM","SOC_ASP"),
                      names_to="PSR",
                      values_to="order") %>%
  filter(order==1) %>%
  select(-order) %>%
  mutate(PSR=ifelse(PSR==matches(PEC_OI[i],ignore.case=FALSE),1,0))

outcome <- "PSR"
covars <- colnames(SL_data)[-which(names(SL_data) == outcome | names(SL_data) == "StudySubjectID")
                            ]
#covars <- covars[1:6]

# create the sl3 task
SL_task <- make_sl3_Task(
  data = SL_data,
  covariates = covars,
  outcome = outcome
)

SL_task

length(SL_task$folds) # how many folds?

head(SL_task$folds[[1]]$training_set) # row indexes for fold 1 training

head(SL_task$folds[[1]]$validation_set) # row indexes for fold 1 validation

any(
  SL_task$folds[[1]]$training_set %in% SL_task$folds[[1]]$validation_set
)

sl3_list_learners("binomial")

# choose base learners and stack them
lrn_glm <- make_learner(Lrnr_glm)#

lrn_mean <- Lrnr_mean$new()
lrn_lasso <- make_learner(Lrnr_glmnet) # alpha default is 1
lrn_polspline <- Lrnr_polspline$new() # model size too big 
lrn_ranger100 <- make_learner(Lrnr_ranger, num.trees = 100)
lrn_hal_faster <- Lrnr_hal9001$new()#
xgb_50 <- Lrnr_xgboost$new(nrounds = 50)
grid_params <- list(
  max_depth = c(2, 4, 6),
  eta = c(0.001, 0.1, 0.3),
  nrounds = 100
)
grid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)
grid
xgb_learners <- apply(grid, MARGIN = 1, function(tuning_params) {
  do.call(Lrnr_xgboost$new, as.list(tuning_params))
})
do.call(c,xgb_learners)
c(xgb_learners[[1]][[1]],xgb_learners[[2]][[1]])

lrn_ridge <- Lrnr_glmnet$new(alpha = 0)
lrn_enet.5 <- make_learner(Lrnr_glmnet, alpha = 0.5)

stack <- make_learner(
  Stack, lrn_mean,lrn_hal_faster,lrn_lasso,lrn_ridge,lrn_enet.5,
  xgb_50,lrn_ranger100
  )#

#'all of the learners will train in a cross-validated manner.
cv_stack <- Lrnr_cv$new(stack)

#screening?

#meta learner
sl <- make_learner(Lrnr_sl, learners = stack)#ensemble sl

discrete_sl_metalrn <- Lrnr_cv_selector$new()#discrete sl is the cv selector
discrete_sl <- Lrnr_sl$new(
  learners = stack,
  metalearner = discrete_sl_metalrn
)

#train SL
sl_fit <- sl$train(SL_task)

# we did it! now we have SL predictions
sl_preds <- sl_fit$predict()
head(sl_preds)
sl_preds=unlist(sl_preds, recursive = F)
# preds=do.call(rbind,sl_preds)
# head(preds)
# colnames(preds)=c("1","2","3","4")

# summary 
sl_fit$cv_risk(loss_fun = loss_loglik_binomial)
#AUC
pROC::multiclass.roc(response=SL_data$PSR,
                     predictor=sl_preds)#as.integer(pred_glmnet) pred_glmnet[,,]
pROC::auc(response=SL_data$PSR,predictor=sl_preds)#as.integer(pred_glmnet) pred_glmnet[,,]
# > library(ROCR) # for AUC calculation
# > 
#   > AUC <- performance(prediction(sl_preds, SL_data$PSR), measure = "auc")@y.values[[1]]
# > plot(performance(prediction(sl_preds, SL_data$PSR), "tpr", "fpr"))


#CV SL
SL_task_new <- make_sl3_Task(
  data = SL_data,
  covariates = covars,
  outcome = outcome,
  folds = origami::make_folds(SL_data, fold_fun = folds_vfold, V = 2)
)
CVsl <- CV_lrnr_sl(
  lrnr_sl = sl_fit, task = SL_task_new, loss_fun = loss_loglik_binomial
)
CVsl %>%
  kable(digits = 4) %>%
  kableExtra:::kable_styling(fixed_thead = T) %>%
  scroll_box(width = "100%", height = "300px")




#Importance analysis
SLdata_varimp <- importance(sl_fit, 
                            fold_number = "validation",
                            loss = loss_loglik_binomial, 
                            importance_metric = "difference",
                            type = "permute")
SLdata_varimp %>%
  kableExtra::kable(digits = 4) %>%
  kableExtra::kable_styling(fixed_thead = TRUE) %>%
  kableExtra::scroll_box(width = "100%", height = "300px")

}

```

## 7-d ML quality of fit

``` {r}

pROC::auc(as.vector(Y),pred_glmnet)
AUC(pred_glmnet,as.vector(Y))#same as above

#Fit with hal
hal_fit <-fit_hal(X = X, Y = Y, family = "binomial",
                  fit_control =  ,
                  #formula = ~ h(.),# + h(., .) +h(.,.,.)
                  
                  #reduce_basis = 0.3,
                  #smoothness_orders = 0,#corresponds to a piece-wise polynomial fit of degree k
                  #num_knots  = 2 #specify the number of knots for each/all interaction degree(s)
)
pred_hal <- predict(hal_fit, new_data = X)

mean((pred_hal - Y)^2)

auc(as.vector(Y),pred_hal)
s<-summary(hal_fit)


#multinomial fit
#fit with lasso
glmnet_fit<-glmnet(X,Y, family = "multinomial")
cvfit <- cv.glmnet(as.matrix(X), Y, family="multinomial",nfolds = 10,type.measure = "deviance")#class or deviance
pred_glmnet <- predict(glmnet_fit, as.matrix(X), type="class",#class","response"
                                  s = cvfit$lambda.min)
pROC::multiclass.roc(response=Y,predictor=as.integer(pred_glmnet) )#as.integer(pred_glmnet) pred_glmnet[,,]
tmp <- data.frame(response=Y,predictor=as.integer(pred_glmnet) )#as.integer(pred_glmnet) pred_glmnet[,,]
tmp <- tmp %>%
  mutate(equal=case_when(response==predictor~1,
                         response!=predictor~0
                         
                         ))
mean(tmp$equal)
```

# 8-cluster

#https://github.com/Statology/R-Guides/blob/main/hierarchical_clustering.R
#https://www.statology.org/hierarchical-clustering-in-r/

```{r}

#load data
df <- USArrests

#remove rows with missing values
df <- na.omit(df)

#scale each variable to have a mean of 0 and sd of 1
df <- scale(df)

#define linkage methods
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

#function to compute agglomerative coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

#calculate agglomerative coefficient for each clustering linkage method
sapply(m, ac)

#perform hierarchical clustering using Ward's minimum variance
clust <- agnes(df, method = "ward")

#produce dendrogram
pltree(clust, cex = 0.6, hang = -1, main = "Dendrogram") 

#calculate gap statistic for each number of clusters (up to 10 clusters)
gap_stat <- clusGap(df, FUN = hcut, nstart = 25, K.max = 10, B = 50)

#produce plot of clusters vs. gap statistic
fviz_gap_stat(gap_stat)

#compute distance matrix
d <- dist(df, method = "euclidean")

#perform hierarchical clustering using Ward's method
final_clust <- hclust(d, method = "ward.D2" )

#cut the dendrogram into 4 clusters
groups <- cutree(final_clust, k=4)

# Number of members in each cluster
table(groups)

#append cluster labels to original data
final_data <- cbind(USArrests, cluster = groups)

#display first six rows of final data
head(final_data)

#find mean values for each cluster
aggregate(final_data, by=list(cluster=final_data$cluster), mean)




# 2. Fuse observations into clusters.
# 
# At each step in the algorithm, fuse together the two observations that are most similar into a single cluster. Each step basically repeats this procedure until all observations are members of one large cluster. The end result is a tree, which can be plotted as a dendrogram. 
# 
# To determine how many clusters the observations should be grouped in, we can use a metric known as the gap statistic, which compares the total intra-cluster variation for different values of k with their expected values for a distribution with no clustering.
# 
# We’ll choose to group our observations into the number of clusters for which the gap statistic is highest.
 

final_data <- cbind(USArrests, cluster = groups)
final_data=cbind(final_data,id=rownames(final_data))
final_data=tidyr::pivot_longer(final_data, cols=Murder:Rape)
final_data=dplyr::arrange(final_data,cluster)
final_data=dplyr::group_by(final_data,cluster)

ggplot(final_data,aes(x=id,y=name, fill=value)) +
geom_tile()+
facet_wrap(~cluster)
#Reprendre les commandes des graphs de scientific reports

```

# 9-tmle/lmtp

```{r}
library(tmle)
system.time(
  tmle_baseline <- tmleMSM(Y=design_matrix$Y,
                     A=design_matrix$bin_KOFGI,
                     W=select(design_matrix,c(unemploy:haqi #unemploy or KOFGI_ctrd
                                             #,age_num,sex_num #no!!!
                                             #,c(location_Albania:location_Zimbabwe)#location FE 
                                             #,c(year_1992:year_2019)# year FE
                                             ,Quality,Y_0#,pct_pop
                                             )),
                     #T=design_matrix$year-1990,
                     #V=matrix(1, ncol=1, nrow=nrow(design_matrix)),#if no V
                     V=select(design_matrix,sex_num),#c(sex_num, age_num)
                     id=design_matrix$location,#correlation within units
                     
        Q.SL.library = all_libraries,#all_libraries, little_libraries
        Qbounds = c(0, Inf),
        g.SL.library = all_libraries,#all_libraries,# little_libraries
        MSM="A*V",#"A*V*T"
        verbose=TRUE,
        V_SL = 20
        )
  )


#### using lmtp
#check Rmd from
#[1] "C:/Users/Guillaume/Desktop/Recoverit 2022-04-17 at 22.09.53/PROJECTS/2022/sAP Globalization/LMTP"


##########

final_matrix_quality_2019$categ_KOFGI2=factor(final_matrix_quality_2019$categ_KOFGI,
                                              ordered=TRUE)
final_matrix_quality_2019$categ_KOFGI2=final_matrix_quality_2019$categ_KOFGI-1
final_matrix_quality_2019$KOFGI2=final_matrix_quality_2019$KOFGI-min(final_matrix_quality_2019$KOFGI)

###using causal glm

mean_lrnr <- Lrnr_mean$new()
fglm_lrnr <- Lrnr_glm_fast$new()
step.f_lrnr <- Lrnr_glm$new()
ranger_lrnr <- Lrnr_randomForest$new()#ranger does not work very well
glmnet_lrnr <- Lrnr_glmnet$new()
mars_lrnr <- Lrnr_earth$new()
# SL for the outcome regression
sl_reg_lrnr <- Lrnr_sl$new(
  learners = list(mean_lrnr,step.f_lrnr, fglm_lrnr, glmnet_lrnr, mars_lrnr),
  metalearner = Lrnr_nnls$new()
)
sl3_Learner_A <- Lrnr_cv$new(sl_reg_lrnr, full_fit = TRUE)
sl3_Learner_Y <- Lrnr_cv$new(sl_reg_lrnr, full_fit = TRUE)


out <- npglm(
  ~1, 
  treatment_level = c(0,1,2,3),
  data = final_matrix_quality_2019,
  W = W, 
  A = "categ_KOFGI2", 
  Y = "log_Y",
  estimand = "TSM",
  sl3_Learner_A=sl_reg_lrnr,
  sl3_Learner_Y=sl_reg_lrnr,
  #cross_fit = TRUE,
  # learning_method = "glm",
)
out


out2 <- contglm(
  formula_continuous = ~ 1 ,
  formula_binary =  ~ -1,#RAS NULL 1 0 vs. -1
  data = final_matrix_quality_2019,
  W = W, 
  A = "categ_KOFGI2", Y = "log_Y",
  estimand = "CATE",
  sl3_Learner_A=sl_reg_lrnr,#sl_reg_lrnr
  sl3_Learner_Y=sl_reg_lrnr,
  # learning_method = "glm",
)
summary(out2)
##########
```

# 10- Classification other than with SuperLearner
http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/

```{r}

library(tidyverse)
library(caret)

data("iris")
# Split the data into training (80%) and test set (20%)
set.seed(123)
training.samples <- iris$Species %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data <- iris[training.samples, ]
test.data <- iris[-training.samples, ]

#Normalize the data. Categorical variables are automatically ignored.
# Estimate preprocessing parameters
preproc.param <- train.data %>% 
  preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
train.transformed <- preproc.param %>% predict(train.data)
test.transformed <- preproc.param %>% predict(test.data)

library(MASS)
# Fit the model
model <- lda(Species~., data = train.transformed)
model
# Coefficients of linear discriminants: Shows the linear combination of predictor variables that are used to form the LDA decision rule. for example, LD1 = 0.91*Sepal.Length + 0.64*Sepal.Width - 4.08*Petal.Length - 2.3*Petal.Width. Similarly, LD2 = 0.03*Sepal.Length + 0.89*Sepal.Width - 2.2*Petal.Length - 2.6*Petal.Width.

# Make predictions
predictions <- model %>% predict(test.transformed)
# Model accuracy
mean(predictions$class==test.transformed$Species)
```

# Next ...
```{r}
# tmp<-read.csv("C:/Users/Guillaume/Downloads/essaiexport.csv")
# tmp2<-tmp %>% 
#   filter(!grepl('ARC', Study.Subject.ID)) #%>%
#   #select(starts_with(match = "LISTE_"))#contains, #matches
# 
# all_PEC<-read.csv(file="G:/_DATA/CRR/REHABase/REHABASE_PEC_allPEC.csv",
#                   header=TRUE, sep=",")
# all_PEC=subset(all_PEC,select=c("StudySubjectID","ProtocolID","LISTE_DOMAINE_SOIN",
#                                 "MODAL_PEC","NB_HEURES","DATE_DEB"))
# 
# First_Network_REHABASE_tot = read.csv(file="G:/_DATA/CRR/REHABase/extract15062021_csv.csv",
#                                       header=TRUE, sep="\t")
# 
# 
# remain_1 <- tmp2 %>%
#   anti_join(all_PEC,by=c("Study.Subject.ID"="StudySubjectID")) 
# remain_1_1 <-remain_1 %>%
#   select(-c(Protocol.ID,Person.ID)) %>%
#   pivot_longer(cols=-Study.Subject.ID,
#                names_to="set",
#                values_to="LISTE_DOMAINE_SOIN",
#                values_drop_na=TRUE
#                ) %>%
#   filter(LISTE_DOMAINE_SOIN %in% c(2,4,5,6,7,8,9,11,13,15))
# 
# colSums(!is.na(select(remain_1,-c(Study.Subject.ID,Protocol.ID,Person.ID))))
# 
# 
# remain_2 <- tmp2 %>%
#   anti_join(First_Network_REHABASE_tot,by="Study.Subject.ID")
###
```
